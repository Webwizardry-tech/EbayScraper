{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bc8e36-a8c5-49be-9f18-99fcc2b64f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import urllib.parse\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "countryDict = {\n",
    "    'au': '.com.au',\n",
    "    'at': '.at',\n",
    "    'be': '.be',\n",
    "    'ca': '.ca',\n",
    "    'ch': '.ch',\n",
    "    'de': '.de',\n",
    "    'es': '.es',\n",
    "    'fr': '.fr',\n",
    "    'hk': '.com.hk',\n",
    "    'ie': '.ie',\n",
    "    'it': '.it',\n",
    "    'my': '.com.my',\n",
    "    'nl': '.nl',\n",
    "    'nz': '.co.nz',\n",
    "    'ph': '.ph',\n",
    "    'pl': '.pl',\n",
    "    'sg': '.com.sg',\n",
    "    'uk': '.co.uk',\n",
    "    'us': '.com',\n",
    "}\n",
    "\n",
    "conditionDict = {\n",
    "    'all': '',\n",
    "    'new': '&LH_ItemCondition=1000',\n",
    "    'opened': '&LH_ItemCondition=1500',\n",
    "    'refurbished': '&LH_ItemCondition=2500',\n",
    "    'used': '&LH_ItemCondition=3000'\n",
    "}\n",
    "\n",
    "typeDict = {\n",
    "    'all': '&LH_All=1',\n",
    "    'auction': '&LH_Auction=1',\n",
    "    'bin': '&LH_BIN=1',\n",
    "    'offers': '&LH_BO=1'\n",
    "}\n",
    "\n",
    "def Items(url,Keyword):\n",
    "    \n",
    "    soup = __GetHTML(url)\n",
    "    data = __ParseItems(soup,Keyword)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def __GetHTML(url):\n",
    "\n",
    "    # Get the web page HTML\n",
    "    request = urllib.request.urlopen(url)\n",
    "    soup = BeautifulSoup(request.read(), 'html.parser')\n",
    "\n",
    "    return soup\n",
    "\n",
    "def __ParseItems(soup,Keyword):\n",
    "    rawItems = soup.find_all('div', {'class': 's-item__info clearfix'})\n",
    "    data = []\n",
    "\n",
    "    for item in rawItems[1:]:\n",
    "        \n",
    "        #Get item data\n",
    "        title = item.find(class_=\"s-item__title\").find('span').get_text(strip=True)\n",
    "        \n",
    "        price = __ParseRawPrice(item.find('span', {'class': 's-item__price'}).get_text(strip=True))\n",
    "        \n",
    "        url = item.find('a')['href']\n",
    "\n",
    "        itemData = {\n",
    "            'Product': title,\n",
    "            'price': price,\n",
    "            'URL': url,\n",
    "            'Keyword':Keyword,\n",
    "        }\n",
    "        \n",
    "        data.append(itemData)\n",
    "    \n",
    "    # Remove item with prices too high or too low\n",
    "    return data\n",
    "\n",
    "def __ParsePrices(soup):\n",
    "    \n",
    "    # Get item prices\n",
    "    rawPriceList = [price.get_text(strip=True) for price in soup.find_all(class_=\"s-item__price\")]\n",
    "    priceList = [price for price in map(lambda rawPrice:__ParseRawPrice(rawPrice), rawPriceList) if price != None]\n",
    "    \n",
    "    # Get shipping prices\n",
    "    rawShippingList = [item.get_text(strip=True) for item in soup.find_all(class_=\"s-item__shipping s-item__logisticsCost\")]\n",
    "    shippingList = map(lambda rawPrice:__ParseRawPrice(rawPrice), rawShippingList)\n",
    "    shippingList = [0 if price == None else price for price in shippingList]\n",
    "\n",
    "    # Remove prices too high or too low\n",
    "    priceList = __StDevParse(priceList)\n",
    "    shippingList = __StDevParse(shippingList)\n",
    "\n",
    "    data = {\n",
    "        'price-list': priceList,\n",
    "        'shipping-list': shippingList\n",
    "    }\n",
    "    return data\n",
    "\n",
    "def __ParseRawPrice(string):\n",
    "    parsedPrice = re.search('(\\d+(.\\d+)?)', string.replace(',', '.'))\n",
    "    if (parsedPrice):\n",
    "        return float(parsedPrice.group())\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def __Average(numberList):\n",
    "\n",
    "    if len(list(numberList)) == 0: return 0\n",
    "    return sum(numberList) / len(list(numberList))\n",
    "\n",
    "def __StDev(numberList):\n",
    "    \n",
    "    if len(list(numberList)) <= 1: return 0\n",
    "    \n",
    "    nominator = sum(map(lambda x: (x - sum(numberList) / len(numberList)) ** 2, numberList))\n",
    "    stdev = (nominator / ( len(numberList) - 1)) ** 0.5\n",
    "\n",
    "    return stdev\n",
    "\n",
    "def __StDevParse(numberList):\n",
    "    \n",
    "    avg = __Average(numberList)\n",
    "    stdev = __StDev(numberList)\n",
    "    \n",
    "    # Remove prices too high or too low; Accept Between -1 StDev to +1 StDev\n",
    "    numberList = [nmbr for nmbr in numberList if (avg + stdev >= nmbr >= avg - stdev)]\n",
    "\n",
    "    return numberList\n",
    "\n",
    "\n",
    "def __GetaAllClasses(soup):\n",
    "    classes = set()\n",
    "    for element in soup.find_all(True, class_=True):\n",
    "        classes.update(element.get('class'))\n",
    "\n",
    "    return classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ed336fa5-25d1-4622-a4d8-2c18cebe566f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been saved to Files/OutputFile.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import urllib\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Specify the path to your Excel file (without the ~$ prefix)\n",
    "excel_file_path = 'Files/Ebaycom-Input.xlsx'\n",
    "output_file_path = 'Files/OutputFile.xlsx'\n",
    "\n",
    "# Read the Excel file into a DataFrame\n",
    "df = pd.read_excel(excel_file_path)\n",
    "\n",
    "# Assuming the links are in a column named 'URL' and keywords in 'Keyword'\n",
    "links = df['URL'].tolist()\n",
    "keywords = df['Keyword'].tolist()\n",
    "\n",
    "data = []  # List to hold all the processed data\n",
    "for link,keyword in zip(links,keywords):\n",
    "    item_data = Items(link,keyword)\n",
    "    \n",
    "    data.append(item_data)\n",
    "\n",
    "# Convert the collected data into a DataFrame\n",
    "result_df = pd.DataFrame(data)\n",
    "\n",
    "# Save the DataFrame to an Excel file\n",
    "result_df.to_excel(output_file_path, index=False)  # index=False to not write row indices\n",
    "\n",
    "print(\"Data has been saved to\", output_file_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
